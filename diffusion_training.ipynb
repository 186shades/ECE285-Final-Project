{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "rIvAR3xDWSc9",
        "outputId": "4924ef23-fb07-42d3-c314-0b20f3466183"
      },
      "outputs": [],
      "source": [
        "import traceback\n",
        "import logging\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import random\n",
        "import torchvision.utils as tvu\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import torch.utils\n",
        "import torch.utils.data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "model_image_size = 256\n",
        "beta_start = 0.0001\n",
        "beta_end = 0.02\n",
        "num_diffusion_timesteps = 1000\n",
        "\n",
        "model_out_ch = 3\n",
        "model_ch = 128\n",
        "model_ch_mult = [1, 1, 2, 2, 4, 4]\n",
        "model_num_res_blocks = 2\n",
        "model_attn_resolutions = [16, ]\n",
        "model_dropout = 0.0\n",
        "model_in_channels = 3\n",
        "model_resamp_with_conv = True\n",
        "\n",
        "sampling_batch_size = 1\n",
        "\n",
        "data_num_workers = 32\n",
        "data_channels = 3\n",
        "\n",
        "time_travel_T_sampling = 100\n",
        "time_travel_travel_length = 1\n",
        "time_travel_travel_repeat = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NwMyQCXmnYeV"
      },
      "outputs": [],
      "source": [
        "project_path = \"ECE285-Final-Project/\"\n",
        "gt_path = \"gt_train\"\n",
        "mask_path = \"mask_train\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKvvPT_FYtzZ",
        "outputId": "ff29832b-9fa7-4b13-e78a-5758ab8b6ff0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Using device: {}\".format(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9w3JodyCZEGc"
      },
      "outputs": [],
      "source": [
        "# set random seed\n",
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(1234)\n",
        "\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5760\n",
            "(tensor([[[0.5569, 0.5843, 0.5490,  ..., 0.3765, 0.3725, 0.3725],\n",
            "         [0.5843, 0.6078, 0.5569,  ..., 0.3765, 0.3765, 0.3765],\n",
            "         [0.6078, 0.6235, 0.5686,  ..., 0.3765, 0.3765, 0.3765],\n",
            "         ...,\n",
            "         [0.1804, 0.1804, 0.1804,  ..., 0.8471, 0.8471, 0.8471],\n",
            "         [0.1804, 0.1804, 0.1804,  ..., 0.8431, 0.8471, 0.8471],\n",
            "         [0.1804, 0.1804, 0.1804,  ..., 0.8431, 0.8431, 0.8431]],\n",
            "\n",
            "        [[0.5098, 0.5373, 0.5098,  ..., 0.5451, 0.5412, 0.5412],\n",
            "         [0.5373, 0.5608, 0.5176,  ..., 0.5451, 0.5451, 0.5451],\n",
            "         [0.5608, 0.5765, 0.5294,  ..., 0.5451, 0.5451, 0.5451],\n",
            "         ...,\n",
            "         [0.1843, 0.1843, 0.1843,  ..., 0.8431, 0.8431, 0.8431],\n",
            "         [0.1843, 0.1843, 0.1843,  ..., 0.8392, 0.8431, 0.8431],\n",
            "         [0.1843, 0.1843, 0.1843,  ..., 0.8392, 0.8392, 0.8392]],\n",
            "\n",
            "        [[0.4078, 0.4353, 0.4039,  ..., 0.5686, 0.5647, 0.5647],\n",
            "         [0.4353, 0.4588, 0.4118,  ..., 0.5686, 0.5686, 0.5686],\n",
            "         [0.4588, 0.4745, 0.4235,  ..., 0.5686, 0.5686, 0.5686],\n",
            "         ...,\n",
            "         [0.1647, 0.1647, 0.1647,  ..., 0.8235, 0.8235, 0.8235],\n",
            "         [0.1647, 0.1647, 0.1647,  ..., 0.8196, 0.8235, 0.8235],\n",
            "         [0.1647, 0.1647, 0.1647,  ..., 0.8196, 0.8196, 0.8196]]]), tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         ...,\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]]]), 'part1_3_0152')\n"
          ]
        }
      ],
      "source": [
        "class ImageMaskDataset(Dataset):\n",
        "  def __init__(self, image_dir, mask_dir, transform=None):\n",
        "    self.image_dir = image_dir\n",
        "    self.mask_dir = mask_dir\n",
        "    self.transform = transform\n",
        "    self.image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]\n",
        "    self.mask_paths = [os.path.join(mask_dir, os.path.basename(f).split('.')[0] + '.png') for f in self.image_paths]  # Assuming mask filenames match image filenames (except for extension)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.image_paths)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    image_path = self.image_paths[idx]\n",
        "    mask_path = self.mask_paths[idx]\n",
        "\n",
        "    # print(image_path)\n",
        "    # print(mask_path)\n",
        "\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    mask = Image.open(mask_path).convert('L')\n",
        "\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "      mask = self.transform(mask)  # You might need a different transform for masks\n",
        "\n",
        "    return image, mask, os.path.basename(image_path).split('.')[0]\n",
        "\n",
        "image_dir = os.path.join(project_path, gt_path)\n",
        "mask_dir = os.path.join(project_path, mask_path)\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize([model_image_size, model_image_size]),\n",
        "                                        transforms.ToTensor()])\n",
        "\n",
        "image_mask_dataset = ImageMaskDataset(image_dir, mask_dir, transform)\n",
        "print(len(image_mask_dataset))\n",
        "print(image_mask_dataset[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AiPE63S-aeqJ"
      },
      "outputs": [],
      "source": [
        "def get_timestep_embedding(timesteps, embedding_dim):\n",
        "    assert len(timesteps.shape) == 1\n",
        "\n",
        "    half_dim = embedding_dim // 2\n",
        "    emb = math.log(10000) / (half_dim - 1)\n",
        "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n",
        "    emb = emb.to(device=timesteps.device)\n",
        "    emb = timesteps.float()[:, None] * emb[None, :]\n",
        "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
        "    if embedding_dim % 2 == 1:  # zero pad\n",
        "        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n",
        "    return emb\n",
        "\n",
        "\n",
        "def nonlinearity(x):\n",
        "    return x*torch.sigmoid(x) # swish\n",
        "\n",
        "\n",
        "def Normalize(in_channels):\n",
        "    return torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "25F2VTL-bBc6"
      },
      "outputs": [],
      "source": [
        "class Upsample(nn.Module):\n",
        "    def __init__(self, in_channels, with_conv):\n",
        "        super().__init__()\n",
        "        self.with_conv = with_conv\n",
        "        if self.with_conv:\n",
        "            self.conv = torch.nn.Conv2d(in_channels,\n",
        "                                        in_channels,\n",
        "                                        kernel_size=3,\n",
        "                                        stride=1,\n",
        "                                        padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.interpolate(\n",
        "            x, scale_factor=2.0, mode=\"nearest\")\n",
        "        if self.with_conv:\n",
        "            x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, in_channels, with_conv):\n",
        "        super().__init__()\n",
        "        self.with_conv = with_conv\n",
        "        if self.with_conv:\n",
        "            self.conv = torch.nn.Conv2d(in_channels,\n",
        "                                        in_channels,\n",
        "                                        kernel_size=3,\n",
        "                                        stride=2,\n",
        "                                        padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.with_conv:\n",
        "            pad = (0, 1, 0, 1)\n",
        "            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0)\n",
        "            x = self.conv(x)\n",
        "        else:\n",
        "            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, *, in_channels, out_channels=None, conv_shortcut=False,\n",
        "                 dropout, temb_channels=512):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        out_channels = in_channels if out_channels is None else out_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.use_conv_shortcut = conv_shortcut\n",
        "\n",
        "        self.norm1 = Normalize(in_channels)\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels,\n",
        "                                     out_channels,\n",
        "                                     kernel_size=3,\n",
        "                                     stride=1,\n",
        "                                     padding=1)\n",
        "        self.temb_proj = torch.nn.Linear(temb_channels,\n",
        "                                         out_channels)\n",
        "        self.norm2 = Normalize(out_channels)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.conv2 = torch.nn.Conv2d(out_channels,\n",
        "                                     out_channels,\n",
        "                                     kernel_size=3,\n",
        "                                     stride=1,\n",
        "                                     padding=1)\n",
        "        if self.in_channels != self.out_channels:\n",
        "            if self.use_conv_shortcut:\n",
        "                self.conv_shortcut = torch.nn.Conv2d(in_channels,\n",
        "                                                     out_channels,\n",
        "                                                     kernel_size=3,\n",
        "                                                     stride=1,\n",
        "                                                     padding=1)\n",
        "            else:\n",
        "                self.nin_shortcut = torch.nn.Conv2d(in_channels,\n",
        "                                                    out_channels,\n",
        "                                                    kernel_size=1,\n",
        "                                                    stride=1,\n",
        "                                                    padding=0)\n",
        "\n",
        "    def forward(self, x, temb):\n",
        "        h = x\n",
        "        h = self.norm1(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv1(h)\n",
        "\n",
        "        h = h + self.temb_proj(nonlinearity(temb))[:, :, None, None]\n",
        "\n",
        "        h = self.norm2(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.dropout(h)\n",
        "        h = self.conv2(h)\n",
        "\n",
        "        if self.in_channels != self.out_channels:\n",
        "            if self.use_conv_shortcut:\n",
        "                x = self.conv_shortcut(x)\n",
        "            else:\n",
        "                x = self.nin_shortcut(x)\n",
        "\n",
        "        return x+h\n",
        "\n",
        "\n",
        "class AttnBlock(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.norm = Normalize(in_channels)\n",
        "        self.q = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.k = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.v = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.proj_out = torch.nn.Conv2d(in_channels,\n",
        "                                        in_channels,\n",
        "                                        kernel_size=1,\n",
        "                                        stride=1,\n",
        "                                        padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_ = x\n",
        "        h_ = self.norm(h_)\n",
        "        q = self.q(h_)\n",
        "        k = self.k(h_)\n",
        "        v = self.v(h_)\n",
        "\n",
        "        # compute attention\n",
        "        b, c, h, w = q.shape\n",
        "        q = q.reshape(b, c, h*w)\n",
        "        q = q.permute(0, 2, 1)  \n",
        "        k = k.reshape(b, c, h*w) \n",
        "        w_ = torch.bmm(q, k)   \n",
        "        w_ = w_ * (int(c)**(-0.5))\n",
        "        w_ = torch.nn.functional.softmax(w_, dim=2)\n",
        "\n",
        "        # attend to values\n",
        "        v = v.reshape(b, c, h*w)\n",
        "        w_ = w_.permute(0, 2, 1) \n",
        "        h_ = torch.bmm(v, w_)\n",
        "        h_ = h_.reshape(b, c, h, w)\n",
        "\n",
        "        h_ = self.proj_out(h_)\n",
        "\n",
        "        return x+h_\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        ch, out_ch, ch_mult = model_ch, model_out_ch, tuple(model_ch_mult)\n",
        "        num_res_blocks = model_num_res_blocks\n",
        "        attn_resolutions = model_attn_resolutions\n",
        "        dropout = model_dropout\n",
        "        in_channels = model_in_channels\n",
        "        resolution = model_image_size\n",
        "        resamp_with_conv = model_resamp_with_conv\n",
        "\n",
        "        self.ch = ch\n",
        "        self.temb_ch = self.ch*4\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.resolution = resolution\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        # timestep embedding\n",
        "        self.temb = nn.Module()\n",
        "        self.temb.dense = nn.ModuleList([\n",
        "            torch.nn.Linear(self.ch,\n",
        "                            self.temb_ch),\n",
        "            torch.nn.Linear(self.temb_ch,\n",
        "                            self.temb_ch),\n",
        "        ])\n",
        "\n",
        "        # downsampling\n",
        "        self.conv_in = torch.nn.Conv2d(in_channels,\n",
        "                                       self.ch,\n",
        "                                       kernel_size=3,\n",
        "                                       stride=1,\n",
        "                                       padding=1)\n",
        "\n",
        "        curr_res = resolution\n",
        "        in_ch_mult = (1,)+ch_mult\n",
        "        self.down = nn.ModuleList()\n",
        "        block_in = None\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_in = ch*in_ch_mult[i_level]\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                block.append(ResnetBlock(in_channels=block_in,\n",
        "                                         out_channels=block_out,\n",
        "                                         temb_channels=self.temb_ch,\n",
        "                                         dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(AttnBlock(block_in))\n",
        "            down = nn.Module()\n",
        "            down.block = block\n",
        "            down.attn = attn\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                down.downsample = Downsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res // 2\n",
        "            self.down.append(down)\n",
        "\n",
        "        # middle\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n",
        "                                       out_channels=block_in,\n",
        "                                       temb_channels=self.temb_ch,\n",
        "                                       dropout=dropout)\n",
        "        self.mid.attn_1 = AttnBlock(block_in)\n",
        "        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n",
        "                                       out_channels=block_in,\n",
        "                                       temb_channels=self.temb_ch,\n",
        "                                       dropout=dropout)\n",
        "\n",
        "        # upsampling\n",
        "        self.up = nn.ModuleList()\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            skip_in = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                if i_block == self.num_res_blocks:\n",
        "                    skip_in = ch*in_ch_mult[i_level]\n",
        "                block.append(ResnetBlock(in_channels=block_in+skip_in,\n",
        "                                         out_channels=block_out,\n",
        "                                         temb_channels=self.temb_ch,\n",
        "                                         dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(AttnBlock(block_in))\n",
        "            up = nn.Module()\n",
        "            up.block = block\n",
        "            up.attn = attn\n",
        "            if i_level != 0:\n",
        "                up.upsample = Upsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res * 2\n",
        "            self.up.insert(0, up) \n",
        "            \n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in,\n",
        "                                        out_ch,\n",
        "                                        kernel_size=3,\n",
        "                                        stride=1,\n",
        "                                        padding=1)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        assert x.shape[2] == x.shape[3] == self.resolution\n",
        "\n",
        "        # timestep embedding\n",
        "        temb = get_timestep_embedding(t, self.ch)\n",
        "        temb = self.temb.dense[0](temb)\n",
        "        temb = nonlinearity(temb)\n",
        "        temb = self.temb.dense[1](temb)\n",
        "\n",
        "        # downsampling\n",
        "        hs = [self.conv_in(x)]\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                h = self.down[i_level].block[i_block](hs[-1], temb)\n",
        "                if len(self.down[i_level].attn) > 0:\n",
        "                    h = self.down[i_level].attn[i_block](h)\n",
        "                hs.append(h)\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                hs.append(self.down[i_level].downsample(hs[-1]))\n",
        "\n",
        "        # middle\n",
        "        h = hs[-1]\n",
        "        h = self.mid.block_1(h, temb)\n",
        "        h = self.mid.attn_1(h)\n",
        "        h = self.mid.block_2(h, temb)\n",
        "\n",
        "        # upsampling\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                h = self.up[i_level].block[i_block](\n",
        "                    torch.cat([h, hs.pop()], dim=1), temb)\n",
        "                if len(self.up[i_level].attn) > 0:\n",
        "                    h = self.up[i_level].attn[i_block](h)\n",
        "            if i_level != 0:\n",
        "                h = self.up[i_level].upsample(h)\n",
        "\n",
        "        # end\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Model(\n",
              "  (temb): Module(\n",
              "    (dense): ModuleList(\n",
              "      (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "      (1): Linear(in_features=512, out_features=512, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (down): ModuleList(\n",
              "    (0-1): 2 x Module(\n",
              "      (block): ModuleList(\n",
              "        (0-1): 2 x ResnetBlock(\n",
              "          (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (temb_proj): Linear(in_features=512, out_features=128, bias=True)\n",
              "          (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (attn): ModuleList()\n",
              "      (downsample): Downsample(\n",
              "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
              "      )\n",
              "    )\n",
              "    (2): Module(\n",
              "      (block): ModuleList(\n",
              "        (0): ResnetBlock(\n",
              "          (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (temb_proj): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nin_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): ResnetBlock(\n",
              "          (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (temb_proj): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (attn): ModuleList()\n",
              "      (downsample): Downsample(\n",
              "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
              "      )\n",
              "    )\n",
              "    (3): Module(\n",
              "      (block): ModuleList(\n",
              "        (0-1): 2 x ResnetBlock(\n",
              "          (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (temb_proj): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (attn): ModuleList()\n",
              "      (downsample): Downsample(\n",
              "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
              "      )\n",
              "    )\n",
              "    (4): Module(\n",
              "      (block): ModuleList(\n",
              "        (0): ResnetBlock(\n",
              "          (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (temb_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nin_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): ResnetBlock(\n",
              "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (temb_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (attn): ModuleList(\n",
              "        (0-1): 2 x AttnBlock(\n",
              "          (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "          (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (downsample): Downsample(\n",
              "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))\n",
              "      )\n",
              "    )\n",
              "    (5): Module(\n",
              "      (block): ModuleList(\n",
              "        (0-1): 2 x ResnetBlock(\n",
              "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (temb_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (attn): ModuleList()\n",
              "    )\n",
              "  )\n",
              "  (mid): Module(\n",
              "    (block_1): ResnetBlock(\n",
              "      (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (temb_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (attn_1): AttnBlock(\n",
              "      (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "      (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "    (block_2): ResnetBlock(\n",
              "      (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (temb_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (up): ModuleList(\n",
              "    (0): Module(\n",
              "      (block): ModuleList(\n",
              "        (0-2): 3 x ResnetBlock(\n",
              "          (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "          (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (temb_proj): Linear(in_features=512, out_features=128, bias=True)\n",
              "          (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nin_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (attn): ModuleList()\n",
              "    )\n",
              "    (1): Module(\n",
              "      (block): ModuleList(\n",
              "        (0): ResnetBlock(\n",
              "          (norm1): GroupNorm(32, 384, eps=1e-06, affine=True)\n",
              "          (conv1): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (temb_proj): Linear(in_features=512, out_features=128, bias=True)\n",
              "          (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nin_shortcut): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1-2): 2 x ResnetBlock(\n",
              "          (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "          (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (temb_proj): Linear(in_features=512, out_features=128, bias=True)\n",
              "          (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nin_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (attn): ModuleList()\n",
              "      (upsample): Upsample(\n",
              "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (2): Module(\n",
              "      (block): ModuleList(\n",
              "        (0-1): 2 x ResnetBlock(\n",
              "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "          (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (temb_proj): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nin_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): ResnetBlock(\n",
              "          (norm1): GroupNorm(32, 384, eps=1e-06, affine=True)\n",
              "          (conv1): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (temb_proj): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nin_shortcut): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (attn): ModuleList()\n",
              "      (upsample): Upsample(\n",
              "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (3): Module(\n",
              "      (block): ModuleList(\n",
              "        (0): ResnetBlock(\n",
              "          (norm1): GroupNorm(32, 768, eps=1e-06, affine=True)\n",
              "          (conv1): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (temb_proj): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nin_shortcut): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1-2): 2 x ResnetBlock(\n",
              "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "          (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (temb_proj): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nin_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (attn): ModuleList()\n",
              "      (upsample): Upsample(\n",
              "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (4): Module(\n",
              "      (block): ModuleList(\n",
              "        (0-1): 2 x ResnetBlock(\n",
              "          (norm1): GroupNorm(32, 1024, eps=1e-06, affine=True)\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (temb_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nin_shortcut): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): ResnetBlock(\n",
              "          (norm1): GroupNorm(32, 768, eps=1e-06, affine=True)\n",
              "          (conv1): Conv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (temb_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nin_shortcut): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (attn): ModuleList(\n",
              "        (0-2): 3 x AttnBlock(\n",
              "          (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "          (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (upsample): Upsample(\n",
              "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (5): Module(\n",
              "      (block): ModuleList(\n",
              "        (0-2): 3 x ResnetBlock(\n",
              "          (norm1): GroupNorm(32, 1024, eps=1e-06, affine=True)\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (temb_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nin_shortcut): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (attn): ModuleList()\n",
              "      (upsample): Upsample(\n",
              "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "  (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = Model().to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DiffusionTrain(nn.Module):\n",
        "    def __init__(self, model, image_resolution=[32, 32, 3], n_times=1000, beta_minmax=[1e-4, 2e-2], device='cuda'):\n",
        "    \n",
        "        super(DiffusionTrain, self).__init__()\n",
        "    \n",
        "        self.n_times = n_times\n",
        "        self.img_H, self.img_W, self.img_C = image_resolution\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "        beta_1, beta_T = beta_minmax\n",
        "        betas = torch.linspace(start=beta_1, end=beta_T, steps=n_times).to(device)\n",
        "        self.sqrt_betas = torch.sqrt(betas)\n",
        "                                     \n",
        "        self.alphas = 1 - betas\n",
        "        self.sqrt_alphas = torch.sqrt(self.alphas)\n",
        "        alpha_bars = torch.cumprod(self.alphas, dim=0)\n",
        "        self.sqrt_one_minus_alpha_bars = torch.sqrt(1-alpha_bars)\n",
        "        self.sqrt_alpha_bars = torch.sqrt(alpha_bars)\n",
        "        \n",
        "        self.device = device\n",
        "    \n",
        "    def extract(self, a, t, x_shape):\n",
        "        b, *_ = t.shape\n",
        "        out = a.gather(-1, t)\n",
        "        return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
        "    \n",
        "    def scale_to_minus_one_to_one(self, x):\n",
        "        return x * 2 - 1\n",
        "    \n",
        "    def reverse_scale_to_zero_to_one(self, x):\n",
        "        return (x + 1) * 0.5\n",
        "    \n",
        "    def make_noisy(self, x_zeros, t): \n",
        "        epsilon = torch.randn_like(x_zeros).to(self.device)\n",
        "        \n",
        "        sqrt_alpha_bar = self.extract(self.sqrt_alpha_bars, t, x_zeros.shape)\n",
        "        sqrt_one_minus_alpha_bar = self.extract(self.sqrt_one_minus_alpha_bars, t, x_zeros.shape)\n",
        "        noisy_sample = x_zeros * sqrt_alpha_bar + epsilon * sqrt_one_minus_alpha_bar\n",
        "    \n",
        "        return noisy_sample.detach(), epsilon\n",
        "    \n",
        "    \n",
        "    def forward(self, x_zeros):\n",
        "        x_zeros = self.scale_to_minus_one_to_one(x_zeros)\n",
        "        \n",
        "        B, _, _, _ = x_zeros.shape\n",
        "        \n",
        "        t = torch.randint(low=0, high=self.n_times, size=(B,)).long().to(self.device)\n",
        "        \n",
        "        perturbed_images, epsilon = self.make_noisy(x_zeros, t)\n",
        "        \n",
        "        pred_epsilon = self.model(perturbed_images, t)\n",
        "        \n",
        "        return perturbed_images, epsilon, pred_epsilon\n",
        "    \n",
        "    \n",
        "    def denoise_at_t(self, x_t, timestep, t):\n",
        "        B, _, _, _ = x_t.shape\n",
        "        if t > 1:\n",
        "            z = torch.randn_like(x_t).to(self.device)\n",
        "        else:\n",
        "            z = torch.zeros_like(x_t).to(self.device)\n",
        "        \n",
        "        epsilon_pred = self.model(x_t, timestep)\n",
        "        \n",
        "        alpha = self.extract(self.alphas, timestep, x_t.shape)\n",
        "        sqrt_alpha = self.extract(self.sqrt_alphas, timestep, x_t.shape)\n",
        "        sqrt_one_minus_alpha_bar = self.extract(self.sqrt_one_minus_alpha_bars, timestep, x_t.shape)\n",
        "        sqrt_beta = self.extract(self.sqrt_betas, timestep, x_t.shape)\n",
        "        \n",
        "        x_t_minus_1 = 1 / sqrt_alpha * (x_t - (1-alpha)/sqrt_one_minus_alpha_bar*epsilon_pred) + sqrt_beta*z\n",
        "        \n",
        "        return x_t_minus_1.clamp(-1., 1)\n",
        "                \n",
        "    def sample(self, N):\n",
        "        x_t = torch.randn((N, self.img_C, self.img_H, self.img_W)).to(self.device)\n",
        "        \n",
        "        for t in range(self.n_times-1, -1, -1):\n",
        "            timestep = torch.tensor([t]).repeat_interleave(N, dim=0).long().to(self.device)\n",
        "            x_t = self.denoise_at_t(x_t, timestep, t)\n",
        "        \n",
        "        x_0 = self.reverse_scale_to_zero_to_one(x_t)\n",
        "        \n",
        "        return x_0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(256, 256, 3)\n"
          ]
        }
      ],
      "source": [
        "# Training hyperparameters\n",
        "epochs = 10\n",
        "train_batch_size = 2\n",
        "lr = 1e-4\n",
        "n_timesteps = 1000\n",
        "beta_minmax=[1e-4, 2e-2]\n",
        "img_size = (model_image_size, model_image_size, 3)\n",
        "print(img_size)\n",
        "\n",
        "diffusion = DiffusionTrain(model, image_resolution=img_size, n_times=n_timesteps, \n",
        "                      beta_minmax=beta_minmax, device=device).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of model parameters:  113673219\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"Number of model parameters: \", count_parameters(diffusion))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Start Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start/Resume training...\n",
            "\tEpoch 11 complete \tDenoising Loss:  0.010113696743216681\n",
            "\tEpoch 12 complete \tDenoising Loss:  0.00984151044353374\n",
            "\tEpoch 13 complete \tDenoising Loss:  0.00954508614598607\n",
            "\tEpoch 14 complete \tDenoising Loss:  0.009957448180479677\n",
            "\tEpoch 15 complete \tDenoising Loss:  0.009263797882424157\n",
            "Training complete\n"
          ]
        }
      ],
      "source": [
        "print(\"Start/Resume training...\")\n",
        "latest_model_path = 'models_new/trained_model_256r_10.pth'\n",
        "epoch_init = 10\n",
        "epochs_left = 5\n",
        "model.load_state_dict(torch.load(os.path.join(project_path, latest_model_path), map_location=device))\n",
        "model.train()\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=image_mask_dataset, batch_size=train_batch_size, shuffle=True, num_workers=1, pin_memory=True)\n",
        "\n",
        "denoising_loss = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(epochs_left):\n",
        "    noise_prediction_loss = 0\n",
        "    for batch_idx, (x, mask, _) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x = x.to(device)\n",
        "        \n",
        "        noisy_input, epsilon, pred_epsilon = diffusion(x)\n",
        "        loss = denoising_loss(pred_epsilon, epsilon)\n",
        "        \n",
        "        noise_prediction_loss += loss.item()\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    print(\"\\tEpoch\", epoch + 1 + epoch_init, \"complete\", \"\\tDenoising Loss: \", noise_prediction_loss / batch_idx)\n",
        "    torch.save(model.state_dict(), f\"ECE285-Final-Project/models_new/trained_model_256r_{epoch + 1 + epoch_init}.pth\")\n",
        "    \n",
        "print(\"Training complete\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
